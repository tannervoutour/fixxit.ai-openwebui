version: '3.8'

services:
  fixxit-openwebui:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: fixxit-openwebui
    ports:
      - "3000:8080"
    environment:
      # Basic configuration
      - WEBUI_NAME=Fixxit.ai
      - WEBUI_URL=http://localhost:3000
      
      # Authentication & Security
      - WEBUI_AUTH=True
      - WEBUI_SECRET_KEY=your-secret-key-here-change-this
      
      # Database (SQLite with persistence)
      - DATABASE_URL=sqlite:///app/backend/data/webui.db
      
      # AI Model Configuration
      - OLLAMA_BASE_URL=http://host.docker.internal:11434
      - OPENAI_API_BASE_URL=https://api.openai.com/v1
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      
      # File uploads and storage
      - ENABLE_RAG=true
      - CHUNK_SIZE=1500
      - CHUNK_OVERLAP=100
      
      # UI Customization
      - DEFAULT_THEME=dark
      - SHOW_ADMIN_DETAILS=false
      
      # Performance
      - WEBUI_BUILD_HASH=fixxit-custom
      
    volumes:
      # Persistent data
      - fixxit_openwebui_data:/app/backend/data
      
      # Custom configurations (optional)
      - ./custom/configs:/app/backend/data/configs
      
      # Custom assets (optional) 
      - ./custom/assets:/app/backend/static/assets
      
    extra_hosts:
      - "host.docker.internal:host-gateway"
    
    restart: unless-stopped
    
    # Resource limits (adjust as needed)
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 512M

volumes:
  fixxit_openwebui_data:
    driver: local

# Optional: Add Ollama service if you want to run it together
# services:
#   ollama:
#     image: ollama/ollama:latest
#     container_name: fixxit-ollama
#     ports:
#       - "11434:11434"
#     volumes:
#       - fixxit_ollama_data:/root/.ollama
#     restart: unless-stopped
#
# volumes:
#   fixxit_ollama_data:
#     driver: local